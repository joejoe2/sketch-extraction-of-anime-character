{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport glob\nfrom matplotlib import pyplot as plt\nimport random\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for tensorflow config\nprint(tf.__version__)\nprint(tf.executing_eagerly())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create tensorboard log file\nwriter = tf.summary.create_file_writer(\"loss\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# get all train img path\nimgs = glob.glob('../input/anime-sketch-colorization-pair/data/train/*.png')\n# num of trian img\nlen(imgs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random draw a img, we want to convert left half to right\nplt.imshow(\n    tf.keras.preprocessing.image.load_img(random.choice(imgs))\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize each color pixel with 8 bit(0-255) to -1 ~ +1\ndef normalize(image):\n    image = (tf.cast(image, tf.float32)/255-0.5)*2\n    return image\n\n# read and split img to return (input,ans) pair\ndef load_image(path):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    \n    w = tf.shape(image)[1]  # img width\n    w = int(w/2)  # the input and ans are in the right and left half\n    \n    # split input and ans\n    input_ans = image[:, w:, :] # sketch\n    input_image = image[:, :w, :] # colored\n    \n    # convert to size 256*256\n    input_ans = tf.image.resize(input_ans, (256, 256))\n    input_image = tf.image.resize(input_image, (256, 256))\n    \n    return normalize(input_image), normalize(input_ans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load training img from path \ndataset = tf.data.Dataset.from_tensor_slices(imgs)\ndataset = dataset.map(load_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define batch size for training\nBCTCH_SIZE = 32\nBUFFER_SIZE = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare training data set\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BCTCH_SIZE)\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\nprint(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot a sample from training data set\nfor img, ans in dataset.take(1):\n    plt.subplot(1, 2, 1)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(img[0]))\n    plt.subplot(1, 2, 2)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(ans[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all test img path\nimgs_test = glob.glob('../input/anime-sketch-colorization-pair/data/val/*.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load test img from path \ndataset_test = tf.data.Dataset.from_tensor_slices(imgs_test)\ndataset_test = dataset_test.map(load_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare test data set\ndataset_test = dataset_test.batch(BCTCH_SIZE)\nprint(dataset_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot a sample from test data set\nfor img, ans in dataset_test.take(1):\n    plt.subplot(1, 2, 1)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(img[0]))\n    plt.subplot(1, 2, 2)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(ans[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import tf model and layers\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Dropout, BatchNormalization, LeakyReLU, ReLU, Activation, Input, Concatenate, UpSampling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import Sequential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for down size conv layer\ndef down_step(filters, size, apply_bn=True):\n    model = Sequential()\n    model.add(Conv2D(filters, size, strides=2, padding='same', use_bias=False))\n    \n    if apply_bn:\n        model.add(BatchNormalization())\n    model.add(LeakyReLU())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for up size deconv layer\ndef up_step(filters, size, apply_drop=False):\n    model = Sequential()\n    model.add(Conv2DTranspose(filters, size, strides=2, padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    \n    if apply_drop:\n        model.add(Dropout(0.3))\n    model.add(ReLU())\n    return model\n\n# for up size interporlation layer\ndef up_step_re_conv(filters, size, apply_drop=False):\n    model = Sequential()\n    model.add(UpSampling2D(size=(4, 4), interpolation='nearest'))\n    model.add(Conv2D(filters, size, strides=2, padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    \n    if apply_drop:\n        model.add(Dropout(0.3))\n    model.add(ReLU())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the generator for gan\ndef Generator():\n    inputs = Input(shape=(256, 256, 3)) #256*256\n    \n    # decoder\n    d1 = down_step(64, (5,5), apply_bn=False)(inputs) #128*128\n    d2 = down_step(128, (3,3))(d1) #64*64\n    d3 = down_step(128, (3,3))(d2) #32*32\n    d4 = down_step(256, (3,3))(d3) #16*16\n    d5 = down_step(256, (3,3))(d4) #8*8\n    d6 = down_step(512, (3,3))(d5) #4*4\n    d7 = down_step(512, (3,3))(d6) #2*2\n    d8 = down_step(512, (3,3))(d7) #1*1\n    \n    # encoder\n    u1 = up_step(512, (3,3), apply_drop=True)(d8) #2*2\n    c2 = Concatenate()([u1, d7])\n    u3 = up_step(512, (3,3), apply_drop=True)(c2) #4*4\n    c4 = Concatenate()([u3, d6])\n    u5 = up_step(256, (3,3), apply_drop=True)(c4) #8*8\n    c6 = Concatenate()([u5, d5])\n    u7 = up_step(256, (3,3))(c6) #16*16\n    c8 = Concatenate()([u7, d4])\n    u9 = up_step(128, (3,3))(c8) #32*32\n    c10 = Concatenate()([u9, d3])\n    u11 = up_step(128, (3,3))(c10) #64*64\n    c12 = Concatenate()([u11, d2])\n    u13 = up_step(64, (5,5))(c12) #128*128\n    c14 = Concatenate()([u13, d1])\n    \n    #x = UpSampling2D(size=(4, 4), interpolation='nearest')(c14) #512*512\n    #x = Conv2D(3, (2,2), strides=2, padding='same', use_bias=False, activation='tanh')(x) #256*256\n    # x = BatchNormalization()(x)\n    \n    x = Conv2DTranspose(3, (2,2), strides=2,activation='tanh')(c14) #256*256\n    return Model(inputs=inputs, outputs=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build and show the generator\ngenerator_C2S = Generator()\ntf.keras.utils.plot_model(generator_C2S, to_file='generator.png',rankdir='TB', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show info of generator\nprint(generator_C2S.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the discriminator for gan\ndef Disc():\n    inp = tf.keras.layers.Input(shape=(256, 256, 3))\n    tar = tf.keras.layers.Input(shape=(256, 256, 3))\n    \n    x = tf.keras.layers.concatenate([inp, tar]) #size=(256, 256, 6)\n    \n    x = down_step(128, (5,5), apply_bn=False)(x) #64 128*128\n    x = down_step(256, (3,3))(x) #256 64*64\n    x = down_step(512, (3,3))(x) #512 32*32\n    x = tf.keras.layers.Conv2D(512, (3,3), strides=1, padding='same',use_bias=False)(x) #512 32*32\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU()(x)\n    \n    x = tf.keras.layers.Conv2D(1, (3,3), strides=1)(x)   #1 30*30 featuremap\n    \n    return tf.keras.Model(inputs=[inp, tar], outputs=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build and show discriminator\ndiscriminator_S = Disc()\ntf.keras.utils.plot_model(discriminator_S, to_file='discriminator.png',rankdir='LR', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show info of the discriminator\nprint(discriminator_S.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LAMBDA = 10  # the bigger is more important for generator to learn the real image\n\n# for generator loss\ndef gen_loss(generator_out_to_discriminator_out, generator_out, target):\n    # generator out(fake) apply to discriminator and try to reach true(ones)\n    gen_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(generator_out_to_discriminator_out), generator_out_to_discriminator_out, from_logits=True)\n    # generator out(fake) compare to real image\n    l1_loss = tf.reduce_mean(tf.abs(target - generator_out))\n    \n    return gen_loss, l1_loss,gen_loss + LAMBDA*l1_loss\n\n# for generator loss using square\ndef gen_loss_ls(generator_out_to_discriminator_out, generator_out, target):\n    # generator out(fake) apply to discriminator and try to reach true(ones)\n    gen_loss =  tf.reduce_mean(tf.math.squared_difference(generator_out_to_discriminator_out, 1))\n    # generator out(fake) compare to real image\n    l1_loss = tf.reduce_mean(tf.abs(target - generator_out))\n    \n    return gen_loss, l1_loss,(gen_loss + LAMBDA*l1_loss)\n\n#for discriminator loss\ndef disc_loss(real_to_discriminator_out, generator_out_to_discriminator_out):\n    # real image apply to discriminator and try to reach true(ones)\n    real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_to_discriminator_out), real_to_discriminator_out, from_logits=True)\n    # generator out(fake) apply to discriminator and try to reach false(ones)\n    g_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(generator_out_to_discriminator_out), generator_out_to_discriminator_out, from_logits=True)\n    \n    return real_loss + g_loss\n\n#for discriminator loss using square\ndef disc_loss_ls(real_to_discriminator_out, generator_out_to_discriminator_out):\n    # real image apply to discriminator and try to reach true(ones)\n    real_loss =  tf.reduce_mean(tf.math.squared_difference(real_to_discriminator_out, 1))\n    # generator out(fake) apply to discriminator and try to reach false(ones)\n    g_loss =  tf.reduce_mean(tf.math.square(generator_out_to_discriminator_out))\n    \n    return (real_loss + g_loss)/2\n\n# use generator to colorize from input sketch and draw the comparison\ndef show_predict_and_compare(model, input_img, target):\n    prediction = model(input_img, training=True)\n    plt.figure(figsize=(15,15))\n\n    display = [input_img[0], prediction[0], target[0]]\n    title = ['input', 'predict', 'real']\n\n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n    # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build the optimizer\ngenerator_C2S_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_S_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define customized training step, this is called per batch\n@tf.function\ndef train_step(input_image, target):\n    sketch = target\n    colored = input_image\n    \n    # compute loss\n    with tf.GradientTape() as gen_C2S_tape, tf.GradientTape() as disc_S_tape:\n        generator_C2S_output = generator_C2S(colored, training=True)#\n        \n        real_to_discriminator_S_out = discriminator_S([colored, sketch], training=True)#\n        \n        generator_C2S_out_to_discriminator_S_out = discriminator_S([colored, generator_C2S_output], training=True)#\n        generator_C2S_g_loss, generator_C2S_l1_loss, generator_C2S_total_loss = gen_loss_ls(generator_C2S_out_to_discriminator_S_out, generator_C2S_output, sketch)#\n        discriminator_S_total_loss = disc_loss_ls(real_to_discriminator_S_out, generator_C2S_out_to_discriminator_S_out)#\n        \n        l1_loss = generator_C2S_l1_loss\n        \n    # apply gradients to optimizer and refine model\n    generator_C2S_gradients = gen_C2S_tape.gradient(generator_C2S_total_loss, generator_C2S.trainable_variables)#\n    discriminator_S_gradients = disc_S_tape.gradient(discriminator_S_total_loss, discriminator_S.trainable_variables)#\n    \n    generator_C2S_optimizer.apply_gradients(zip(generator_C2S_gradients, generator_C2S.trainable_variables))#\n    discriminator_S_optimizer.apply_gradients(zip(discriminator_S_gradients, discriminator_S.trainable_variables))#\n    \n    # return loss value for tracking                                                                                    \n    return generator_C2S_g_loss, generator_C2S_total_loss, discriminator_S_total_loss, l1_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train by a loop within 0~epochs and draw test result per 5 epochs\ndef fit(train, test, epochs):\n    # train epochs \n    for epoch in range(0, epochs):\n        print(\"epoch \"+str(epoch)+\":\")\n        \n        # call customized train function per iteration in one epoch\n        for img, ans in train:                                                          \n            generator_C2S_g_loss, generator_C2S_total_loss, discriminator_S_total_loss, l1_loss=train_step(img, ans)\n            print('.', end='')\n        \n        # track loss                                          \n        with writer.as_default():\n            tf.summary.scalar('generator_C2S_total_loss', tf.reduce_mean(generator_C2S_total_loss), step=epoch)#\n            tf.summary.scalar('discriminator_S_total_loss', tf.reduce_mean(discriminator_S_total_loss), step=epoch)#\n            tf.summary.scalar('generator_C2S_g_loss', tf.reduce_mean(generator_C2S_g_loss), step=epoch)#\n            tf.summary.scalar('l1_loss', l1_loss, step=epoch)\n        \n        # draw test result \n        if epoch%5 == 0 or epoch == epochs-1:\n            for img, ans in test.take(3):\n                show_predict_and_compare(generator_C2S, img, ans)\n        print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start training\nEPOCHS = 50\nfit(dataset, dataset_test, EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\ngenerator_C2S.save('generator_C2S.h5')\ndiscriminator_S.save('discriminator_S.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show result in train data set\nfor example_input, example_target in dataset.take(25):\n    show_predict_and_compare(generator_C2S, example_input, example_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show result in test data set\nfor example_input, example_target in dataset_test.take(25):\n    show_predict_and_compare(generator_C2S, example_input, example_target)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}